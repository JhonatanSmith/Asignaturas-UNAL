---
title: "Practica 1 (Notas smith)"
author: "Jhonatan Smith Garcia"
date: "19/11/2021"
output: pdf_document
---
```{r}

library(ISLR)
dim(Auto) # Hay 392 registros y 9 variables
names(Auto) # El nombre de las variables

attach(Auto)

cedula = 33234 # Una semilla de aleatorizacion

set.seed(cedula)
```


```{r}
# Hay 392 datos, osea que la mitad son 196

subset1 = sample((1:nrow(Auto)),196) # Selecciono 196 datos aleatorios (prop es la mitad)

Train1 = Auto[subset1,] # Datos de entrenamiento

Test1 = Auto[-subset1,] # Datps de prueba
```


# La idea es ver la relacion entre mpg (millas por galon) y horsepower
# Caballos de fuerza

```{r}
library(ggplot2)
require(ggplot2)
ggplot()+
  geom_point(data=Auto, aes(horsepower, mpg), color = "blue", size=2)

```

Claramente se ve una relacion entre las variables graficadas

Con este codigo se va a calculoar diferentes MSE de diferentes polinomios
ajustado a esa nube de puntos


```{r}
MSE = vector()
for (i in 1:10){
  
  Modelo1 = lm(mpg~poly(horsepower,i), data = Train1)
  Pred1 = predict(Modelo1, Test1)
  MSE[i] = mean((Pred1-Test1$mpg)^2) #Promedio de lo predecido menos lo real
}
```


```{r}
plot(1:10, MSE, xlab = "Grado polinomio ajustado", ylab = "MSE", type = "b", col=4)
```


Claramente el mejor MSE es el grado de polinomio de grado 2.Esta divisiom de los datos tambien 
puede representar diferencas.

EN GENERAL, DATOS GRANDES SI TRABAJA. NO CIN DATOS PEQUEÑOS

SOBREAJUSTAR: Crear una cosa tan perfecta para mis datos que cuando se vaya a probar
para predecir datos diferente no sirva para nada ese modelo.


# Leave-One-Out Cross validation (LOOCV)

Saque un solo dato de toda la base de datos. Luego, toda la base de datos menos ese punto será el conjunto de entrenamiento.

Ya con esto, utilice ese modelo y lo prueba en su dato que sacó.


Repita el anterior proceso con una observacion 2. Y luego evalue en el dato 2.

Y asi se van a realizar n observaciones y se van a sacar n modelos.

$$C V_{(n)}=\frac{1}{n} \sum_{i=1}^{n} M S E_{i}$$

Este Cv cogerá todos los MSE y los promediará entre si.

OJO CON EL DATO COMPUTANCIONAL. 


# Validacion cruzada K-fold Cross-Validation


Se divide en k pedazos (sub conjuntos) la base de datos. 

por ejemplo, 1000 datos los divide en 10 grupos cada uno de 100.


Luego, tendria k=10 pliegues, de los cuales saco 9 y calculo un modelo con 9 y el que se salio, se utiliza para probar, calculando el MSE.

Luego se utiliza ese seungo pliegue, se tiene un segundo MSE

Y asi, para todos los pliegues.

Empiricamente se escoge k=10


# Validacion cruzada para variables cualitativas

Cuando estabamos en variables Y cuantitativas se usaba el MSE para medir eficiencia

Sin embargo cuando se trabaja con *cualitativas* se utilza es la proporcion de errores obtenidos.

$$E_{r r}=\frac{1}{n_{\text {test }}} \sum_{i=1}^{n_{\text {tes}}} I\left(y_{i} \neq \hat y_{i}\right)$$
Este se conoce como el error de prueba. Cada vez que yo me equivoque en mi base de datos, voy a sumar un 1. 

De esta manera, se tiene que:

$$I\left(y_{i} \neq \hat{y}_{i}\right)= \begin{cases}1, & \text { si } y_{i} \neq \hat{y}_{i} \\ 0, & \text { si } y_{i}=\hat{y}_{i}\end{cases}$$

Lo ideal es que esta tasa de error sea lo mas pequeña posible. 

# Metodo de bootstrap

Basicamente remuestreo. Tomo 10 mil muestras de la base de datos, multiplicarla muchisimas veces. Se muestrea con reemplazo.

SUPER IMPORTANTE PARA CONSTRUIR IC DE LOS R^2 POR EJEMPLO

El error de estimacion para los IC será dado por:

$$S E_{B}(\hat{\alpha})=\sqrt{\frac{1}{B-1} \sum_{r=1}^{B}\left(\hat{\alpha}^{*r}-\frac{1}{B} \sum_{s=1}^{B} \hat{\alpha}^{* s}\right)^{2}}$$

Donde los alfa son los parametros estimados de las B bases de datos obtenias por el metodo de Bootstrap.

Con esto se puede calcular IC para los alfa (parametro de interes)




















