---
title: "Trabajo1_Modulo3"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Punro 3
## Librerías

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ISLR)
library(e1071)
```
## a. Datos
De la libreria ISLR se utilizó la base de datos OJ, la cual se dividio en dos conjuntos, uno de entrenamiento y otro de prueba con 800 y 270 observaciones respectivamente.
```{r message=FALSE}
### Literal a.
datos <- ISLR::OJ
set.seed(1527)
subset<-sample((1:nrow(datos)), 800)
train<-datos[subset,] # 800 datos de entrenamiento
test<-datos[-subset,]
```
## b. Clasificador de soporte vectorial con nucleo lineal

Primero se ajusto un clasificador de soporte vectorial utilizando un costo de 0.1 y un nucleo lineal, este modelo obtuvo el siguiente resumen:
```{r}
### Literal b.
linear <- svm(Purchase~., data=train , kernel = "linear", cost = 0.1,
            scale = FALSE )
summary(linear)
```
De este summary se obtuvo 435 vectores de soporte, 218 en una clase y 217 en la otra.

## c. Tasas de error de entrenamiento y de prueba

Se predicen la etiquetas de clase de las observaciones del conjunto de entrenamiento.
```{r}
### Literal c.
## Tasa de error de entrenamiento
pred_train_l <- predict(linear, train)
table(prediccion = pred_train_l, real = train$Purchase)
```
```{r echo=FALSE, message=FALSE}
ltr <- paste("Observaciones de train mal clasificadas:", 
      100 * mean(train$Purchase != pred_train_l) %>% 
        round(digits = 4), "%")
```
De esta tabla se obtiene que 670 de la observaciones se han clasificado de forma correcta por lo que el 16.25% de las observaciones de entrenamiento estan mal clasificadas.

Ahora, se predicen la etiquetas de clase de las observaciones del conjunto de prueba.
```{r}
## Tasa de error de prueba
pred_test_l <- predict(linear, test)
table(prediccion = pred_test_l, real = test$Purchase)
```
```{r echo=FALSE, message=FALSE}
lts <- paste("Observaciones de test mal clasificadas:", 
             100 * mean(test$Purchase != pred_test_l) %>% 
            round(digits = 4), "%")
```
Se observa que con un costo de 0.1, 216 observaciones se han clasificado correctamente y se obtiene que el 20% de las observaciones de prueba estan mal clasificadas.

## c. Función tune()
Se utiliza la función tune() para obtener el valor óptimo para el parámetro cost, para el cual se considera valores en el rango 0.01 a 10.

```{r}
### Literal d.
tune.linear <- tune(svm, Purchase~., data=train ,kernel ="linear",
                    ranges = list(cost=c(0.01, 0.1, 1,5,10)))
summary(tune.linear)
```
De este summary, se puede apreciar que el menor error es de 0.16375 el cual le corresponde al modelo ajustado con un costo de 10. Ahora, con la linea  "tune.linear$best.model" se obtiene este modelo el cual tiene el siguiente summary:

```{r}
### Literal e.
best_linear <-  tune.linear$best.model
summary(best_linear)
```
de donde se obtuvo 317 vectores de soporte, 160 en una clase y 157 en otra clase. Si se compará con el modelo ajustado inicialmente, se puede apreciar que este utilizó 118 vectores de soporte menos, por lo que se ve una mejora en el ajuste. 

## e. Tasas de error de entrenamiento y prueba para el mejor modelo

Se predicen las etiquetas de clase para las observaciones en el conjunto de entrenamiento.
```{r}
## Tasa de error de entrenamiento
pred_best_train_l <- predict(best_linear, train)
table(prediccion = pred_best_train_l, real = train$Purchase)
```
```{r echo=FALSE, message=FALSE}
ltrb <- paste("Observaciones de train mal clasificadas:", 
              100 * mean(train$Purchase != pred_best_train_l) %>% 
              round(digits = 4), "%")
```
Acá se observa que 676 observaciones fueron clasificadas de manera correcta, obteniendo 6 observaciones más que con el modelo ajustado al principio. Además, el porcentaje de observaciones mal clasificadas es de 15.5% lo cual es menor que el 16.25% del modelo anterior, lo que implica una mejora.
```{r}
## Tasa de error de prueba
pred_best_test_l <- predict(best_linear, test)
table(prediccion = pred_best_test_l, real = test$Purchase)
```
```{r echo=FALSE, message=FALSE}
ltsb <- paste("Observaciones de test mal clasificadas:", 
              100 * mean(test$Purchase != pred_best_test_l) %>% 
              round(digits = 4), "%")
```
Ahora, para las observaciones del conjunto de prueba se tiene que 219 fueron clasificadas de manera correcta y el 18.89% de estas observaciones fueron mal clasificadas, y como con el caso de entrenamiento se presenta una mejora.

## f. Clasificador de soporte vectorial con nucleo radial

A continuación se ajusta un segundo modelo con nucleo radial, con un costo de 0.1 y con el valor por defecto para $\gamma$. Este tiene el siguiente summary el cual obtuvo 535 vertores de soporte, 270 de una clase y 265 en la otra.
```{r}
### Literal f.
radial <- svm(Purchase~., data = train, kernel = "radial", cost = 0.1)
summary(radial)
```
Ahora, se predicen la estiquetas de clase para los conjuntos de entreanamiento y de prueba.
```{r}
## Tasa de error de entrenamiento
pred_train_r <- predict(radial, train)
table(prediccion = pred_train_r, real = train$Purchase)
```
```{r echo=FALSE, message=FALSE}
rtr <- paste("Observaciones de train mal clasificadas:", 
              100 * mean(train$Purchase != pred_train_r) %>% 
              round(digits = 4), "%")
```
Se aprecia que 670 observaciones del conjunto de entrenamiento fueron clasificadas de forma correcta, es decir, el 16.25% de estas fueron clasificadas erroneamente.
```{r}
## Tasa de error de prueba
pred_test_r <- predict(radial, test)
table(prediccion = pred_test_r, real = test$Purchase)
```
```{r echo=FALSE, message=FALSE}
rts <- paste("Observaciones de test mal clasificadas:", 
             100 * mean(test$Purchase != pred_test_r) %>% 
             round(digits = 4), "%")
```
Ahora, con el conjunto de prueba se puede apreciar que 210 observaciones fueron clasificadas correctamente, dejando así el 22.22% de observaciones mal clasificadas.

Acontinuación se procede a usar la función tune() para encontrar el valor óptimo para el parámetro cost.
```{r}
tune.radial <- tune(svm, Purchase~., data=train ,kernel ="radial",
                    ranges = list(cost=c(0.01, 0.1, 1,5,10)))
summary(tune.radial)
```
Del anterior summary se observa que el menor error fue de 0.16625 que corresponde a un cotos de 1. 

A continucación se muestra el summary del modelo ajustado con este costo.
```{r}
best_radial <-  tune.radial$best.model
summary(best_radial)
```
Se puede ver que este modelo obtuvo 359 vectores de soporte, 176 menos que el modelo ajustado con un coste de 0.1. De estos 359 vectores 180 pertenecen a una clase y 179 a la otra.
```{r}
## Tasa de error de entrenamiento
pred_best_train_r <- predict(best_radial, train)
table(prediccion = pred_best_train_r, real = train$Purchase)
```
```{r echo=FALSE, message=FALSE}
rtrb <- paste("Observaciones de train mal clasificadas:", 
              100 * mean(train$Purchase != pred_best_train_r) %>% 
              round(digits = 4), "%")
```
Note que para el modelo ajustado con un costo de 1 obtuvo 682 observaciones del conjunto de entrenamiento cladificadas correctamente, 12 más que con el modelo con un costo de 0.1. Por lo que el 14.75% de estas observaciones fueron mal clasificadas. 
```{r}
## Tasa de error de prueba
pred_best_test_r <- predict(best_radial, test)
table(prediccion = pred_best_test_r, real = test$Purchase)
```
```{r echo=FALSE, message=FALSE}
rtsb <- paste("Observaciones de test mal clasificadas:", 
              100 * mean(test$Purchase != pred_best_test_r) %>% 
              round(digits = 4), "%")
```
Ahora, con el conjunto de prueba se obtuvo que 216 observaciones fueron clasificadas de manera correcta, 6 observaciones mas que el modelo ajustado con un costo 0.1. Donde el 20% fue mal clasificado.

## g. Clasificador de soporte vectorial con nucleo polinomial

Por último, se ajuto un tercer modelo con un nucleo polinomial y un costo de 0.1, el cual arrojo el siguiente summary:
```{r}
### Literal g.
poli <- svm(Purchase~., data = train, kernel = "polynomial", 
            degree = 2, cost = 0.1)
summary(poli)
```
Con este modelo se obtuvo 570 vectores de soporte, 288 en una clase y 282 en la otra.
```{r}
## Tasa de error de entrenamiento
pred_train_p <- predict(poli, train)
table(prediccion = pred_train_p, real = train$Purchase)
```

```{r echo=FALSE, message=FALSE}
ptr <- paste("Observaciones de train mal clasificadas:", 
             100 * mean(train$Purchase != pred_train_p) %>% 
             round(digits = 4), "%")
```
De la anterior tabla se observa que 565 observaciones del conjunto de entrenamiento fueron clasificadas correctamente, dejando así un 29.38% de observaciones mal clasificadas.
```{r}
## Tasa de error de prueba
pred_test_p <- predict(poli, test)
table(prediccion = pred_test_p, real = test$Purchase)
```
```{r echo=FALSE, message=FALSE}
pts <- paste("Observaciones de test mal clasificadas:", 
             100 * mean(test$Purchase != pred_test_p) %>% 
             round(digits = 4), "%")
```
Para el conjunto de prueba se tiene que 175 observaciones fueron clasificadas correctamente, por lo que el 35.79% fue clasificado erroneamente.

A continuación se obtiene el mejor modelo variando el costo en un rango de valores que va de 0.01 hasta 10.
```{r}
tune.poli <- tune(svm, Purchase~., data=train ,kernel ="polynomial",
                    degree = 2,ranges = list(cost=c(0.01, 0.1, 1,5,10)))
summary(tune.poli)
```
Observe del anterior summary que el menor error obtenido es de 0.17500, el cual se logra con un costo de 10. Ahora, se guarda este modelo y se muestra su summary.
```{r}
best_poli <-  tune.poli$best.model
summary(best_poli)
```
De este se obtuvo un total de 334 vectores de soporte donde 171 corresponden a una clase y 163 a la otra.
```{r}
## Tasa de error de entrenamiento
pred_best_train_p <- predict(best_poli, train)
table(prediccion = pred_best_train_p, real = train$Purchase)
```
```{r echo=FALSE, message=FALSE}
ptrb <- paste("Observaciones de train mal clasificadas:", 
              100 * mean(train$Purchase != pred_best_train_p) %>% 
              round(digits = 4), "%")
```
Ahora, se puede apreciar que 692 observaciones del conjunto de prueba fueron clasificadas correctamente, 122 observaciones más que con el modelo incial, por lo que se obtiene que el 15.62% de estas observaciones fueron clasificadas de manera incorrecta.
```{r}
## Tasa de error de prueba
pred_best_test_p <- predict(best_poli, test)
table(prediccion = pred_best_test_p, real = test$Purchase)
```
```{r echo=FALSE, message=FALSE}
ptsb <- paste("Observaciones de test mal clasificadas:", 
              100 * mean(test$Purchase != pred_best_test_p) %>% 
              round(digits = 4), "%")
```
Y del conjunto de prueba se puede ver que 221 observaciones fueron clasificadas de manera correcta, 43 observaciones más que con el modelo incial, dejando un porcentaje de 15.56% observaciones mal clasificadas.

## h. Mejores resultados

En general se observa que con el modelo con nucleo lineal con un coste de 10 se obtiene 317 vectores de soporte, un error de 0.16375, 15.5% de observaciones del conjunto de entrenamiento mal clasificadas y 18.89% de observaciones del conjunto de prueba mal clasificadas. Con el modelo con nucleo radial y un costo de 1 se obtiene 359 vectores de soporte, un error de 0.16625, 14.75% de observaciones del conjunto de entrenamiento mal clasificadas y 20% de observaciones del conjunto de prueba mal clasificadas y con el modelo con nucleo polinomial y un costo de 10 se obtiene 334 vectores de soporte, un error de 0.17500, 15.62% de observaciones del conjunto de entrenamiento mal clasificadas y 15.56% de observaciones del conjunto de prueba mal clasificadas. Por lo tanto, teniendo en cuanta lo anterior, si se usa el conjunto de entrenamiento el método con nucleo radial parece proporcionar los mejores resultados con estos datos, y si se usa el conjunto de prueba, el método con núcleo polinomial es con el que mejor resultados obtiene con estos datos.