---
title: "**Tarea 1_Modulo2**"
author:
- "**Luisa María Acosta O.**"
- "**Sebastian Agudelo J.**"
- "**Laura Camila Agudelo O.**"
- "**Karen Andrea Amaya M.**"
- "**Estefania Echeverry F.**"
date: "**Octubre 2020**"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\textbf{Librerías}

```{r echo=T, results='hide'}
#Librerias
library(kableExtra, warn.conflicts=F, quietly=T)#tablas
library(Hmisc,warn.conflicts=F, quietly=T) #Rezagos
library(class,warn.conflicts=F, quietly=T)#knn
library(caret,warn.conflicts=F, quietly=T) #cv
require(ISLR,warn.conflicts=F, quietly=T)
require(glmnet,warn.conflicts=F, quietly=T) 
#Función para normalizar variables
normalize <- function(x) {
  norm <- ((x - min(x))/(max(x) - min(x)))
  return (norm)
}
```

 \textbf{1.} Descargue de Yahoo Finance la base de datos de los precios de cierre diarios de la acción que se le asignó a su grupo  \textbf{(Grupo 9, HP inc.)} en el periodo que va del 1 de enero de 2015 hasta el 31 de diciembre de 2019


```{r}
datos<- read.csv("HPQ.csv")
datos$Date <- as.Date(datos$Date, format = "%Y-%m-%d")
head(datos, 3) %>% kbl() %>% kable_styling()
```

En primer lugar hacemos un análisis exploratorio de los datos. A continuación podemos ver que la base de datos tiene 1250 observaciones con 7 variables, de las cuales 6 son de tipo factor y una de tipo fecha.

```{r}
str(datos)
```


 \textbf{a.} Contextualice a qué se dedica y dónde opera principalmente la empresa que se le asignó a su grupo. Luego, construya una base de datos con la misma estructura de los datos Smarket que se encuentran en el paquete ISLR (ver diapositiva 29 de la Clase 1). Realice un análisis descriptivo con los resultados y los gráficos que usted considere pertinentes, explicando lo que observa en cada uno.
 
 
\textbf{Contextualización}


HP inc. es una empresa estadounidense líder mundial en dispositivos de computación personal, impresoras, impresión 3D y otros servicios, surgida de la separación, en 2015, de HP (Hewlett-Packard) fundada por Bill Hewlett y Dave Packard en 1939.  Actualmente Dion Weisler es el Presidente y la empresa opera con sede principal en Palo Alto, California.


\textbf{Datos HP inc. tipo Smarket}

 
En las siguientes lineas de código se extraen algunas variables para acercarse a la estructura de la base de datos Smarket. Primero, la variable "Today" puede ser hallada como:

$$Today_{i}=\ln \left(\frac{close_{i}}{close_{i-1}}\right)$$
De esta manera si dicho valor decrece es reemplazado con "down", de lo contrario "up". Luego de la variable "fecha", hacemos uso de la función substr() para extraer sus primeros 4 caracteres, correspondientes al año de interés.

```{r}
Today <- c() 
for(i in 1:nrow(datos)){
  Today[i]<-log(datos$Close[i]/datos$Close[i-1])
}
Direction <- ifelse(Today < 0, "down", "up")
Direction <- as.factor(Direction)
Year <- substr(datos$Date, 1,4)
Year <- as.numeric(Year)
df <- data.frame(Year, "Volume"= datos$Volume, Today, Direction)
```


Ahora se calculan los cinco rezagos, con ayuda de la librería Hmisc:

```{r}
df$lag1 <- Lag(df$Today, 1)
df$lag2 <- Lag(df$Today, 2)
df$lag3 <- Lag(df$Today, 3)
df$lag4 <- Lag(df$Today, 4)
df$lag5 <- Lag(df$Today, 5)
```


De esta manera llegamos a la base de datos HP inc. con la estructura de Smarket:

```{r}
df <- df[-c(1:6),]
head(df)%>% kbl() %>% kable_styling()
```


Como podemos observar el 2015 fue el año con mayor volumen en ventas, con volumenes atípicos muy altos, en los años consecutivos disminuyó el volumen de ventas, aunque se mantuvó al parecer estable, aproximadamente con el mismo promedio en el volumen de ventas.


```{r}
qplot(x = factor(Year),y = Volume, data = df, geom = "boxplot", colour = factor(Year))
```

A continuación se observa un resumen de cada variable.

```{r}
summary(df)
```


 \textbf{b.} Utilizando validación cruzada, encuentre el K (el número de vecinos), del modelo KNN, que mejores resultados arroje en términos del error de prueba estimado para predecir si el precio de la acción sube (o se mantiene igual) o baja en función de los 5 "lags" pasados y el volumen.
 
Primero dividimos la base de datos en entrenamiento y prueba. Por ser datos temporales se prefiere tomar como datos de prueba aquellos del año 2019 y el restante como datos de entrenamiento. 
 
```{r}
train <- (df$Year<2019)
test <- df[!train,] # Datos de test
train <- df[train,] # Datos de train
```

Luego normalizamos la variable Volumen de la siguiente manera:

$$
z=\frac{x-\min (x)}{\max (x)-\min (x)}
$$
```{r}
train$Volume <- normalize(train$Volume)
test$Volume <- normalize(test$Volume)
y_train <- train["Direction"]
y_test <- test["Direction"]
```

Con ayuda de la librería caret, se usa el método cv (Validación Cruzada) para indicar que se va a partir la base de entrenamiento en 5 partes iguales de forma aleatoria (5 fold en inglés). Luego, con su función trainControl se especifican una serie de parámetros en el modelo. El objeto que sale de trainControl se proporcionará como argumento para la función train que se usa para entrenar, de la siguiente manera:

```{r}
set.seed(1233)
SP_ctrl <- trainControl(method="cv", number = 5) #5 fold
SP_knnEntrenado <- train(Direction ~ ., data = train, 
                method = "knn", tuneLength = 20, #numeros de k
                trControl = SP_ctrl,
                preProcess = c("center","scale"))
SP_knnEntrenado
```


Así entonces, la presición del modelo resulta ser óptima asignando el valor de k = 25, como se puede observar en la siguiente gráfica:

```{r}
plot(SP_knnEntrenado)
```

 \textbf{c.} Con los datos de entrenamiento, ajuste un modelo logístico, un KNN con K encontrado en el item (b), un LDA y un QDA para predecir si el precio de la acción sube (o se mantiene igual) o baja en función de los 5 "lags" pasados y el volumen. Para cada modelo obtenga la matriz de confusión y el estimador del error de prueba. Cuál modelo arroja mejores resultados y por qué?
 
 
\textbf{Modelo logístico}


Haciendo uso de la partición de datos del numeral "b" ajustamos el siguiente modelo lógistico, con "Direction" como variable independiente, así entonces, volumen y los cinco rezagos como variables predictoras:

```{r}
train <- (df$Year<2019)
test <- df[!train,] # Datos de test
train <- df[train,] # Datos de train
```


```{r}
mod_glm <- glm(Direction ~ lag1+lag2+lag3+lag4+lag5+Volume, 
               data = df[df$Year<2019,], family = binomial)
```

```{r}
#matriz de confusión
glm.pred <- predict(mod_glm, test[,-4], type="response")
glm.pred <- ifelse(glm.pred < 0.53, "down", "up")
t<-table(glm.pred, test$Direction);t
```

```{r}
#tasa de acierto
sum(diag(t))/sum (t)
```
```{r}
#tasa de error
mean(glm.pred!=test$Direction)
```

Así entonces, la proporción de veces que el modelo logístico predijo correctamente (accuracy) es de del 54.6%. Hay que tener en cuenta que se pueden mejorar estos resultados realizando otro tipo de análisis y limpieza de los datos.


\textbf{Modelo K-Vecinos más cercanos}


Haciendo uso de la partición de datos del numeral "b" ajustamos el siguiente modelo KNN, con "Direction" como variable independiente, así entonces, volumen y los cinco rezagos como variables predictoras:

```{r}
train$Volume <- normalize(train$Volume)
test$Volume <- normalize(test$Volume)
y_train <- train["Direction"]
y_test <- test["Direction"]
```


```{r}
set.seed(1233)
fit.knn_train <- class::knn(train=train[,-4], test= train[,-4], 
                            cl = y_train$Direction, k=25, prob=TRUE)
```

```{r}
fit.knn_Test <- class::knn(train=train[,-4], test = test[,-4], 
                         cl= y_train$Direction, k=25, prob=TRUE)
```

```{r}
#matriz de confusión
Predicted_test<-factor(fit.knn_Test)
t1<-table(Predicted_test,y_test$Direction)
t1
```

```{r}
#tasa de acierto
sum(diag(t1))/sum(t1)
```

```{r}
#tasa de error
mean(Predicted_test!=test$Direction)
```

Así entonces, la proporción de veces que el modelo KNN predijo correctamente (accuracy) es de 79.7%. Hay que tener en cuenta que se pueden mejorar estos resultados realizando otro tipo de análisis y limpieza de los datos.


\textbf{Modelo Análisis de discriminante linea LDA}


Es una alternativa a la regresión logística cuando la variable cualitativa tiene más de dos niveles. Si bien existen extensiones de la regresión logística para múltiples clases, el LDA presenta una serie de ventajas

Haciendo uso de la partición de datos del numeral "b" ajustamos el siguiente modelo LDA, con "Direction" como variable independiente, así entonces, volumen y los cinco rezagos como variables predictoras:

```{r}
train <- (df$Year<2019)
test <- df[!train,] # Datos de test
train <- df[train,] # Datos de train
```

```{r}
mod_lda <- MASS::lda(Direction~lag1+lag2+lag3+lag4+lag5+Volume,data = train)
mod_lda
```

La salida LDA indica que el 47% de las observaciones de entrenamiento corresponden a días en los que el mercado bajó. Los coeficientes de salida de los discriminantes lineales proporcionan la combinación lineal del volumen y los rezagos que se utilizan para formar la regla de decisión LDA.

```{r}
#matriz de confusión
Pred.lda<-predict(mod_lda , test) 
Clase.lda = Pred.lda$class 
t<-table(Clase.lda,test$Direction);t
```

```{r}
#tasa de acierto
sum(diag(t))/sum(t)
```

```{r}
#tasa de error
mean(Clase.lda!=test$Direction)
```

Así entonces, la proporción de veces que el modelo LDA predijo correctamente (accuracy) es de 55.4%. Hay que tener en cuenta que se pueden mejorar estos resultados realizando otro tipo de análisis y limpieza de los datos.


\textbf{Modelo QDA (Análisis discriminante cuadrático)}


Haciendo uso de la partición de datos del numeral "b" ajustamos el siguiente modelo QDA, con "Direction" como variable independiente, así entonces, volumen y los cinco rezagos como variables predictoras:
 
```{r}
mod_qda <- MASS::qda(Direction~lag1+lag2+lag3+lag4+lag5+Volume, data = train)
mod_qda
```


```{r}
#matriz de confusion
Pred.qda<-predict(mod_qda , test) 
Clase.qda =Pred.qda$class
t1<-table(Clase.qda, test$Direction);t1
```

```{r}
#Tasa de acierto
sum(diag(t1))/sum(t1) 
```
 
 
Así entonces, la proporción de veces que el modelo QDA predijo correctamente (accuracy) es del 55.8%. Hay que tener en cuenta que se pueden mejorar estos resultados realizando otro tipo de análisis y limpieza de los datos.


 \textbf{Cuál modelo arroja mejores resultados y por qué?}
 
 - Los modelos logístico, QDA y LDA obtuvieron una tasa de aciertos similar 54.6%, 55.4% y 55.8% respectivamente. La exactitud (accuracy) más alto fue de 79.7% obtenido por el modelo KNN, teniendo en cuenta que este último es más flexible y tuvo la ventaja con respecto a los otros modelos de utilizar validación cruzada para minimizar su error.
 




 \textbf{2. Resuelva el ejercicio 3 de la sección 4.7 del texto}




Este problema se relaciona con el modelo QDA, en el que las observaciones dentro de cada clase se extraen de una distribución normal con un vector medio específico de la clase y una matriz de covarianza específica de la clase. Consideramos el caso simple donde p = 1; es decir, solo hay una característica.


Suponga que tenemos K clases, y que si una observación pertenece a la k-ésima clase, entonces X proviene de una distribución normal unidimensional, X $\sim$ N ( $\mu_k$,  $\sigma_k^2$). Recuerde que la función de densidad para la distribución normal unidimensional se da en (4.11). Demuestre que en este caso, el clasificador de Bayes no es lineal. Argumenta que de hecho es cuadrático.


Sugerencia: Para este problema, debe seguir los argumentos establecidos en la Sección 4.4.2, pero sin asumir que  $\sigma_1^2$ =. . . = $\sigma_K^2$.

$$
p_k(x) = \frac {\pi_k
                \frac {1} {\sqrt{2 \pi} \sigma_k}
                \exp(- \frac {1} {2 \sigma_k^2} (x - \mu_k)^2)
               }
               {\sum_{l = 1}^K{
                \pi_l
                \frac {1} {\sqrt{2 \pi} \sigma_l}
                \exp(- \frac {1} {2 \sigma_l^2} (x - \mu_l)^2)
               }}
$$
$$
\log(p_k(x)) = \frac {\log(\pi_k) +
                \log(\frac {1} {\sqrt{2 \pi} \sigma_k}) + 
                - \frac {1} {2 \sigma_k^2} (x - \mu_k)^2
               }
               {\log(\sum_{l = 1}^K {
                \pi_l
                \frac {1} {\sqrt{2 \pi} \sigma_l}
                \exp(- \frac {1} {2 \sigma_l^2} (x - \mu_l)^2)
               })}
$$

$$
\log(p_k(x)) = \log(\pi_k) +
                \log(\frac {1} {\sqrt{2 \pi} \sigma_k}) 
                - \frac {1} {2 \sigma_k^2} (x - \mu_k)^2
                -
               \log(\sum_{l = 1}^K {
                \pi_l
                \frac {1} {\sqrt{2 \pi} \sigma_l}
                \exp(- \frac {1} {2 \sigma_l^2} (x - \mu_l)^2)
               })
$$


Como el último termino $\log(\sum_{l = 1}^K {
                \pi_l
                \frac {1} {\sqrt{2 \pi} \sigma_l}
                \exp(- \frac {1} {2 \sigma_l^2} (x - \mu_l)^2)
               })$,
es independiente de k, este será un termino constante $\forall$ k pues es una sumatoria sobre K(grande), por lo cual, queda hallar k en el cual se maximiza:


$$
\log(\pi_k) +
  \log(\frac {1} {\sqrt{2 \pi} \sigma_k})  
  - \frac {1} {2 \sigma_k^2} (x - \mu_k)^2
$$

$$
= log(\pi_k) + log(\frac{1}{\sqrt{2\pi}\sigma_k}) -  \frac{x^2}{2\sigma_k^2} + x \cdot \frac{\mu_k}{\sigma_k^2}  - \frac{\mu_k^2}{2\sigma_k} 
$$

Con lo anterior:
$$
\delta_k(x)
= log(\pi_k) + log(\frac{1}{\sqrt{2\pi}\sigma_k}) -  \frac{x^2}{2\sigma_k^2} + x \cdot \frac{\mu_k}{\sigma_k^2}  - \frac{\mu_k^2}{2\sigma_k} 
$$


Como se observa, $\delta_k(x)$ no es una ecuación lineal pues depende de un termino cuadrático en x.

 
**3.**Pruebe que si en el modelo de regresión lineal múltiple se tiene que 
$p > n$ (el número de covariables es mayor que el tamaño muestral) entonces
los estimadores de los coeficientes,$\widehat{\beta }$ , no son únicos. ¿Cuáles son las
alternativas para “resolver” este problema?

Para la regresión lineal múltiple, los coeficientes de regresión se pueden estimar por mínimos cuadrados, Suponiendo lo contrario que se plantea en el punto y  que $n > p $, se tiene a $y_{i}$ la cual es la i-ésima respuesta observada y $x_{ij}$ la i-ésima observación; también se supone que el término del error $\varepsilon$ 
tiene $E\left ( \varepsilon  \right ) = 0$ y $Var\left ( \varepsilon  \right ) = \sigma ^{2}$, y los errores no estan correlacionados.

El modelo muestral de regresión se puede escribir como:


$$
y_{i}=\beta _{0}+\beta _{1}x_{i1}+\beta _{1}x_{i2}+\ ...\ +\beta _{k}x_{ik} +\varepsilon _{i}\\
y_{i}=\beta _{0}+\sum_{j=1}^{k}\beta _{j}x_{ij} +\ \varepsilon _{i},  \ i=1,2,...n
$$
La función de mínimos cuadrados es

$$
S\left ( \beta _{0},\beta _{1}, ..., \beta _{k} \right ) = \sum_{i = 1}^{n}\varepsilon _{i}^{2}\\
 = \sum_{i = 1}^{n}\left ( y_{i}-\beta _{0}-\sum_{j=1}^{k}\beta _{j}x_{ij} \right )^{2}
$$
Se debe minimizar la función $S$ respecto a $\beta _{0},\beta _{1}, ..., \beta _{k}$. Los estimadores de $\beta _{0},\beta _{1}, ..., \beta _{k}$ por mínimos cuadrados deben satisfacer

$$
\frac{\partial S}{\partial \beta _{0}}\mid _{\widehat{\beta _{0}},{\widehat\beta _{2},...,\widehat\beta _{k}}} = -2\sum_{i=1}^{n}\left ( y_{i}-\widehat{\beta _{0}}-\sum_{j=1}^{k}\widehat{\beta_{j}}x_{ij} \right ) = 0\\

\frac{\partial S}{\partial \beta _{j}}\mid _{\widehat{\beta _{0}},{\widehat\beta _{2},...,\widehat\beta _{k}}} = -2\sum_{i=1}^{n}\left ( y_{i}-\widehat{\beta _{0}}-\sum_{j=1}^{k}\widehat{\beta_{j}}x_{ij} \right )x_{ij} = 0, \ j \ = \ 1,2,...,k
$$
Al simplificar la ecuación se obtienen las ecuaciones normales de mínimos cuadrados

$$
n\widehat{\beta _{0}} + \widehat{\beta _{1}}\sum_{i=1}^{n}x_{i1} +  \widehat{\beta _{2}}\sum_{i=1}^{n}x_{i2} + ... +  \widehat{\beta _{k}}\sum_{i=1}^{n}x_{ik} = \sum_{i=1}^{n}y_{i}\\

\widehat{\beta _{0}}\sum_{i=1}^{n}x_{i1} + \widehat{\beta _{1}}\sum_{i=1}^{n}x_{i1}^{2} +  \widehat{\beta _{2}}\sum_{i=1}^{n}x_{i1}x_{i2} + ... +  \widehat{\beta _{k}}\sum_{i=1}^{n}x_{i1}x_{ik} = \sum_{i=1}^{n}x_{i1}y_{i}\\

\vdots  \\


\widehat{\beta _{0}}\sum_{i=1}^{n}x_{ik} + \widehat{\beta _{1}}\sum_{i=1}^{n}x_{ik}x_{i1} +  \widehat{\beta _{2}}\sum_{i=1}^{n}x_{ik}x_{i2} + ... +  \widehat{\beta _{k}}\sum_{i=1}^{n}x_{ik}^2 =\sum_{i=1}^{n}x_{ik}y_{i}\\
$$

Se sae que $p = k + 1$, se mostratra de forma matricial 

$$
y =X\beta + \varepsilon 
$$

Se determina el estimador de $\widehat{\beta}$ por minimos cuadrados:

$$
S(\beta )= \sum_{i=1}^{n}\varepsilon _{i}^2 = \varepsilon {}' \ \varepsilon  = (y - X\beta ){}'(y-X\beta )\\

S(\beta )= y{}'y- \beta {X}'y - y{}'X\beta + \beta {}'X{}'X\beta \\

S(\beta )= y{}'y- 2 \beta {X}'y + \beta {}'X{}'X\beta 
$$
Los estimadores de mínimos cuadrados deben satisfacer 
$$

X{}'X\widehat{\beta }=X{}'y\\
\widehat{\beta }=\left (  X{}'X\right )^{-1}X{}'y
$$
Como se logra ver ese seria el estimador de los $\widehat{\beta}$ y se supone que la matriz inversa 
$ \left ( X{}'X\right )^{-1}$ siempre existe si los regresores son linealmente independientes.

Es decir, la estimación está bien definida cuando las variables predictoras, es decir, las columnas de x, son linealmente independientes. Pero cuando $p> n$, la matriz  $X$ no puede tener columnas linealmente independientes, $
y $X^{T}X$ es no invertible, pues como hay mas covariables hay la posibilidad de multicolinealidad, también  al tener un tamaño de muestra pequeño la regresión lineal múltiple se ajusta a los pocos puntos , dando un problema de ajuste,para poder dar predicción , por esto si el modelo, se utiliza para dar predicciones o se cambia la base de datos, los $\widehat{\beta}$ cambiarian completamente, y no tendrian un rango de estimación, además que su $R^{2}$ podria ser muy cercano a 1 , pero seria una mala estimación.

Hay varias maneras de arreglar las cuales serian:


- Selección de subconjuntos de covariables: Las covariables
más relevantes para explicar la variabilidad de Y son seleccionadas, es decir, las que expliquen la mayor variabilidad de Y, y no haya multicolinealidad.

-Reducción de dimensionalidad: Consiste en proyectar las p
covariables en un subespacio de dimensión reducida, $M$, con $M < p$. Esto produce $M$ combinaciones lineales de las variables originales y dichas combinaciones se usan luego como predictores
o covariables.

- la selección progresiva hacia adelante, la regresión de crestas, el lazo y la regresión de componentes principales, son particularmente útil para realizar regresiones en entornos de alta dimensión, es decir , mayor número de covariables que tamalo de muestra $p > n$. Esencialmente, estos enfoques evitan el sobreajuste mediante el uso de un enfoque de ajuste menos flexible que los mínimos cuadrados.

Al utilizar estos modelos en entornos de alta dimensión, siempre tener en centa:

-La regularización o la contracción juega un papel clave en un entorno de alta dimensión $(p> n)$.

-La selección del parámetro de ajuste apropiado es clave para una buena precisión de predicción

-El error de prueba aumentará a medida que aumente la dimensionalidad del problema.


**4.**

**a.** Se simula una base de datos considerando el modelo lineal $Y=X\beta+\epsilon$, teniendo en cuenta las siguientes condicines:

* $Y$: es una variable numérica dependiente.
* $X_1$, $X_2$ y $X_3$ son variables numéricas independientes.
* $X_4$ es una variable categórica independiente con tres categorías.
* $(\beta_0,\beta_1,\beta_2,\beta_3,\beta_4,\beta_5)^T=(1,0.3,0.6,-1,1.5,-2)^T$
* $\epsilon \sim T-studen(9)$
* $n=500$.

```{r echo=TRUE}
set.seed(123)
# Valores para la simulación
n <- 500

x1 <- runif(n = n, min = 0, max = 1)
x2 <- rpois(n = n, lambda = 3)
x3 <- rgamma(n = n, shape = 3, scale = 2, )
x4 <- c('A', 'B', 'C')
x4 <- sample(x4, n, replace = T)

# Matriz X del modelo
x <- model.matrix(~ x1+x2+x3+x4)

# Vector verdadero de parámetros
beta <- c(1, 0.3, 0.6, -1, 1.5, -2)
error <- rt(n,9)

# Variable respuesta
y <- x %*% beta + error

# Base de datos con la simulación hecha
datos <- data.frame(y, x[,2:6])
```

Ahora, utilizando bootstrap se obtendrán intervalos de confianza al 95% para $\sigma$, $R^2$ y $R^2-Ajustado$.

```{r warning=FALSE}
library(boot)

#IC para sigma 
sigma <- function(datos,i){
  mod = lm(y~x, data = datos, subset = i)
  summary(mod)$sigma
}

boot1 <- boot(datos,sigma,R=500)

boot.ci(boot1,conf = 0.95,type = "all")
```

```{r warning=FALSE}
#IC para R2
r2 <- function (datos,i){
  mod = lm(y~x , data = datos, subset = i)
  summary(mod)$r.squared
}

boot2 <- boot(datos,r2,R=500)

boot.ci(boot2,conf = 0.95,type = "all")
```

```{r warning=FALSE}
#IC para R2 ajustado
r2adj <- function (datos,i){
  mod = lm(y~x , data = datos, subset = i)
  summary(mod)$adj.r.squared
}

boot3 <- boot(datos,r2adj,R=500)

boot.ci(boot3,conf = 0.95, type = "all")
```

Se sabe que los errores siguen una distibucion t-student con 9 grados de libertad, por lo que su varianza se calcula como v/(v-2), donde v son los grados de libertad y esto es para v>2, así, se tiene que la varianza teórica esperada para los errores es:

```{r}
v <- 9   
var_esp <- v/(v-2)   
var_esp
```
**b.** Simulando de nuevo el modelo del literal anterior pero ahora incluyendo un término cúbico para $X_1$ con coeficiente $\beta_{13}=2$, se tiene:

```{r}
set.seed(123)
n <- 500
s <- 2 

x1 <- runif(n = n, min = 0, max = 1)
x2 <- rpois(n = n, lambda = 3)
x3 <- rgamma(n = n, shape = 3, scale = 2, )
x4 <- c('A', 'B', 'C')
x4 <- sample(x4, n, replace = T)

x <- model.matrix(~x1+I(x1^3)+x2+x3+x4)

beta <- c(1,0.3,2,0.6,-1,1.5,-2)

error <- rnorm(n,0,s)

y <- x %*% beta + error

# Base de datos con la simulación hecha
datos <- data.frame(y, x[,2:7])
```

Ahora, utilizando validacion cruzada, se varia de 1 a 10 el grado del polinomio que depende de $X_1$ y se ajusta un modelo para cada uno, además, se calcula el MSE respectivo.

```{r}
p <- 0.70
n <- nrow(datos)
muestra <- sample(1:n, size = p*n)
train <- datos[muestra,]
test <- datos[-muestra,]

y_train <- train$y
y_test <- test$y

#modelo con x1
mod1 <- lm(y~.,data = train)
mse1 <- mean((y_test-predict(mod1, test))^2)

#modelo con x1^2
mod2 <- lm(y~x1+I(x1^2)+x2+x3+x4B+x4C,data = train)
mse2 <- mean((y_test-predict(mod2, test))^2)

#modelo con x1^3
mod3 <- lm(y~x1+I(x1^3)+x2+x3+x4B+x4C,data = train)
mse3 <- mean((y_test-predict(mod3, test))^2)

#modelo con x1^4
mod4 <- lm(y~x1+I(x1^4)+x2+x3+x4B+x4C,data = train)
mse4 <- mean((y_test-predict(mod4, test))^2)

#modelo con x1^5
mod5 <- lm(y~x1+I(x1^5)+x2+x3+x4B+x4C,data = train)
mse5 <- mean((y_test-predict(mod5, test))^2)

#modelo con x1^6
mod6 <- lm(y~x1+I(x1^6)+x2+x3+x4B+x4C,data = train)
mse6 <- mean((y_test-predict(mod6, test))^2)

#modelo con x1^7
mod7 <- lm(y~x1+I(x1^7)+x2+x3+x4B+x4C,data = train)
mse7 <- mean((y_test-predict(mod7, test))^2)

#modeloc con x1^8
mod8 <- lm(y~x1+I(x1^8)+x2+x3+x4B+x4C,data = train)
mse8 <- mean((y_test-predict(mod8, test))^2)

#modelo con x1^9
mod9 <- lm(y~x1+I(x1^9)+x2+x3+x4B+x4C,data = train)
mse9 <- mean((y_test-predict(mod9, test))^2)

#modelo con x1^10
mod10 <- lm(y~x1+I(x1^10)+x2+x3+x4B+x4C,data = train)
mse10 <- mean((y_test-predict(mod10, test))^2)

tabla <- cbind(c(mse1,mse2,mse3,mse4,mse5,mse6,mse7,mse8,mse9,mse10))
colnames(tabla) <- c("MSE")
rownames(tabla) <- c("Grado 1","Grado 2","Grado 3","Grado 4",
                     "Grado 5","Grado 6","Grado 7","Grado 8",
                     "Grado 9","Grado 10")
tabla
```
De la anterior tabla vemos que el modelo ajustado con el polinomio de grado de 10 es el que tiene el menor MSE, por lo cual este es el modelo que se debería escoger. También, se puede ver que el modelo ajustado con el polinomio de grado 1 y el ajustado con el polinomio de grado 3 tienen el mismo MSE. Además, el modelo con el polinomio de grado 2 es el que presenta el mayor MSE. Por último, note que si bien la mayoria de modelos presente diferentes MSE y que a medida que se aumenta el grado del polinomio el MSE disminuye, la diferencia de estos valores no es muy grande.

Ahora, se realiza un gráfico de los MSE en función de dichos grados de libertad.

```{r fig.width=6,fig.height=4}
plot(c(mse1,mse2,mse3,mse4,mse5,mse6,mse7,mse8,mse9,mse10),
     xlab = "Grado del polinomio", ylab = "MSE", las=1,
     ,pch=19,
     col = "purple")
labels <- c("1","2","3","4","5","6","7","8","9","10")
text(c(mse1,mse2,mse3,mse4,mse5,mse6,mse7,mse8,mse9,mse10),labels,
     cex = 0.7, pos = 2)
```

Si se observa el gráfico anterior, se puede apreciar que los resultados coninciden con los esperados, pues se ve que a medida que aumenta el grado del polinomio menor es el MSE, lo cual es razonable pues entre mas alto es el grado del polinomio es más alta la probabilidad de sobreajuste del modelo, por lo tanto, se espera que el error disminuya.

```{r, include=FALSE}
library(olsrr)
require(leaps)

```
**5.** A continuación analizaremos tiempo de supervivencia de la base de datos SURGICAL que contiene datos sobre la supervivencia de pacientes sometidos a operación hepática. La base tiene una dimensión `r dim(surgical)` y contiene las variables:

**bcs:** puntuación de coagulación sanguínea.

**pindex:** índice de pronóstico.

**enzyme_test:** puntuación de la prueba de función enzimática.

**liver_test:** puntuación de la prueba de función hepática.

**age:** edad en años.

**gender:** variable indicadora de género (0 = masculino, 1 = femenino).

**alc_mod:** variable indicadora del historial de consumo de alcohol (0 = Ninguno, 1 = Moderado).

**alc_heavy:** variable indicadora del historial de consumo de alcohol (0 = Ninguno, 1 = Fuerte).

**y:** tiempo de supervivencia.

Para seleccionar las variables que expliquen mejor el tiempo de supervivencia, procedemos a realizar validación cruzada con nuestra base de datos, dividiéndola en don conjuntos de datos, la de entrenamiento con el 70% de los datos, y la de test con el 30% de los datos.
```{r , echo=TRUE}
set.seed (72)
porc <- 0.7 # Porcentaje de datos de entrenamiento
t <- sample (length(surgical$y) ,size = (length(surgical$y)*porc)) 
train <- surgical[t,]
test <- surgical[-t,]

```

con la base *train* utilizamos la función *regsubsets*, para seleccionar las variables con selección hacia adelante.
```{r}
regfit.for <- regsubsets(y~.,data=train,nvmax =8, method = "forward")
``` 
Con selección hacia adelante con los criterios *adjr2* y *bic* obtenemos los siguientes gráficos, que muestran los modelos posibles. Según el *adjr2* el mejor modelo es el `r which.max(summary(regfit.for)$adjr2)` y con el *bic* es el `r which.min(summary(regfit.for)$bic)`.
```{r, echo= FALSE}
par(mfrow=c(1,2), cex=0.7)
plot(regfit.for, scale ="adjr2") 
plot(regfit.for, scale ="bic")
```
Para evaluar los modelos anteriores, se utiliza la función mostrada en la clase 4 *predict.regsubsets* que calcula el MSE para cada uno de los modelos. 

```{r, echo=FALSE }
predict.regsubsets =function (object,newdata,y){
  form<-as.formula(object$call[[2]])
  mat<-model.matrix(form ,newdata)
  val.errors =rep(NA, (ncol(mat)-1))
  for(i in 1:length(val.errors)){
    coefi<-coef(object ,id=i)
    xvars<-names (coefi)
    pred<-mat[,xvars]%*%coefi
    val.errors [i]= mean((y-pred)^2)
  }
  val.errors
}


```

```{r , fig.width=5, fig.height=4,  echo=FALSE}
e1 <- predict.regsubsets(regfit.for,test,test$y)

plot(e1, type = "b")
```
En el grafico anterior podemos ver los MSE calculados con los datos de test, el cual observamos que el menor es `r which.min(e1)`.
Ahora 
```{r}
b1 <- which.min(e1)
regfit.for <- regsubsets(y~.,data=surgical ,nvmax =8, method = "forward")
coef(regfit.for,b1)
```
Así las variables que mejor explican el tiempo de supervivencia son: *bcs, pindex, enzyme_test, liver_test, alc_heavy*, obteniendo el siguente modelo,

`y= -1178.33 + 59.86*bcs + 8.92*pindex + 9.74*enzyme_test + 58.06*liver_test + 317.84*alc_heavy`.


Ahora con el método de selección hacia atrás, con la base de datos *train* procedemos a  la seleccionar las variables, procedemos igual que el método anterior.
```{r}
regfit.bac <- regsubsets(y~.,data=train,nvmax =8, method = "backward")
```
Con selección hacia atrás con los criterios *adjr2* y *bic* obtenemos los siguientes gráficos, que muestran los modelos posibles. Según el *adjr2* el mejor modelo es el `r which.max(summary(regfit.bac)$adjr2)` y con el *bic* es el `r which.min(summary(regfit.bac)$bic)`.
```{r, echo= FALSE}
par(mfrow=c(1,2), cex= 0.7)
plot(regfit.bac, scale ="adjr2") 
plot(regfit.bac, scale ="bic")
```
Utilizando la función *predict.regsubset* obtenemos los MSE, con los cuales obtenemos el siguiente grafico

```{r, fig.width=5,fig.height=4,echo= FALSE}
e2<-predict.regsubsets(regfit.bac,test,test$y)
plot(e2, type = "b")
b2<-which.min(e2)
```

```{r, echo= FALSE}
regfit.bac<-regsubsets(y~.,data=surgical ,nvmax =8, method = "backward")

coef(regfit.bac,b2)
```
Así las variables que mejor explican el tiempo de supervivencia son: *bcs, pindex, enzyme_test, alc_heavy*, obteniendo el siguente modelo,

`y= -1334.42 + 81.43*bcs + 10.13*pindex + 11.24*enzyme_test + 312.77*alc_heavy`.


Ahora con el método de el mejor subconjunto, con la base de datos *train* procedemos a  la seleccionar las variables, procedemos igual que el método anterior

```{r}
regfit <-regsubsets(y~.,data=train,nvmax =8)
```
Con selección del mejor subconjunto con los criterios *adjr2* y *bic* obtenemos los siguientes gráficos, que muestran los modelos posibles. Según el *adjr2* el mejor modelo es el `r which.max(summary(regfit)$adjr2)` y con el *bic* es el `r which.min(summary(regfit)$bic)`.
```{r, echo= FALSE}
par(mfrow=c(1,2), cex=0.7)
plot(regfit, scale ="adjr2") 
plot(regfit, scale ="bic")
```
Utilizando la función *predict.regsubset* obtenemos los MSE, con los cuales obtenemos el siguiente grafico

```{r, echo= FALSE,fig.width=5,fig.height=4}
e3<-predict.regsubsets(regfit,test,test$y)
plot(e3, type = "b")
b3<-which.min(e3)
```
```{r, echo= FALSE}
regfit<-regsubsets(y~.,data=surgical ,nvmax =8)

coef(regfit,b3)
```
Así las variables que mejor explican el tiempo de supervivencia son: *bcs, pindex, enzyme_test, alc_heavy *, obteniendo el siguente modelo,

`y= -1334.42 + 81.43*bcs + 10.13*pindex + 11.24*enzyme_test + 312.77*alc_heavy`.

**6.** Utilizando la base de datos "BASE_DATOS_1" se aplicará las ténicas de regularización ridge y lasso.

#### Ridge regression

```{r echo=FALSE}
require(glmnet) #Libreria que contiene las funciones para aplicar
                #ridge y lasso

#Lectura de la base BASE_DATOS_1
base1 <- read.table("../BASE_DATOS_1.txt",header = T)
```


Se crea una variable x en la cual se guardan todas las covariables que se usaran en el modelo y una variable $Y$ la cual será la variable respuesta que se utilizará.

```{r }
x <- model.matrix(Y~., data = base1)[,-1]
y <- base1$Y

gridz <- 10^seq(-2,10, length=100)
mod <- glmnet(x,y,alpha = 0,lambda = gridz)

dim(coef(mod)) #Numero de filas y columnas del objeto mod
```
En el siguiente gráfico se puede apreciar el comportamiento de los $\lambda$ a lo largo de cada covariable. Se observa el peso de las covariables a la hora de explicar $Y$ y su comportamiento, se ve que los $\beta$ $6$, $9$ y el intercepto parecen tener un mayor peso sobre la respuesta $Y$, y como se espera todos los $\lambda$ tienden a cero a medida que crecen.

```{r fig.width=6,fig.height=4}
plot(mod,xvar="lambda",label=T)
```
Ahora, se dividira la base en datos de prueba y datos de entrenamiento para aplicar validación cruzada y encontrar el $\lambda$ óptimo.

```{r fig.width=6,fig.height=4}
set.seed(123)

train<-sample(1: nrow(x), nrow(x)/2)
test<- -train
y.test<-y[test]

cv.out<-cv.glmnet(x[train,],y[train],alpha=0)

plot(cv.out)
```
Del gráfico anterior se puede apreciar el $log(\lambda)$ en función del MSE de entrenamiento y se ve que a medida que aumenta el $log(\lambda)$ el MSE aumenta, dando así una relación directa entre estos. Para ver explicitamente cual es el valor del $\lambda$ óptimo se usa el siguiente comando:

```{r }
bestlam<-cv.out$lambda.min
bestlam
```
Ahora, se usa la función predict para hallar los valores ajustados y se calcula el MSE de este modelo.

```{r}
ridge.pred<-predict(mod, s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
```
Así, se obtiene un $\lambda = 0.6134$ y un $MSE = 1.2859$ para el modelo ajustado con ridge.

Por último, observamos los valores predichos y se puede apreciar que el valor más grande que se obtiene es de 5.3331, por lo cual, se usa el siguiente criterio para ver cuales son las covariables que podrían explicar mejor a $Y$:

* Se considera que 5.3331 es el valor más grande en este caso, por lo cual se descartan todos los valores predichos menores a 0.0106.

```{r}
out<-glmnet (x,y,alpha=0)
pre <- predict(out,type="coefficients",s=bestlam)[1:1328,]
```

Así, se obtiene que las variables que podrían explicar mejor a la respuesta son las siguientes:

```{r echo=FALSE}
sort(abs(pre),decreasing = T)[1:52]
```

Por lo tanto, las variables que aparentemente muestran no ser relevantes para explicar la variable aleatoria $Y$ son las que no fueron incluidas en el conjunto anterior. 

#### Lasso regression

Ahora, para aplicar regresión lasso, se usa el mismo procedimiento que en regresión ridge, solo que esta vez se usara el parámetro "alpha" igual a 1.

```{r}
lasso.mod<-glmnet(x,y,alpha=1, lambda=gridz)
dim(coef(lasso.mod)) #Numero de columnas y filas en el objeto lasso.mod
```
```{r fig.width=6,fig.height=4}
plot(lasso.mod, xvar="lambda", label=TRUE)
```
Del gráfico anterior se puede apreciar que, al igual que en regresión ridge los $\beta$ 0, 6 y 9 parecen ser los de mayor peso a la hora de explicar $Y$. El comportamiento de los $\lambda$ es parecido y a medida que aumentan tienden a cero.

```{r fig.width=6,fig.height=4}
cv.out<-cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
```
```{r}
bestlam<-cv.out$lambda.min
bestlam
```
De los comandos anteriores y del gráfico se obtienen todos los $\lambda$ y se visualiza un $\lambda$ óptimo de 0.0305.

Ahora, se usa la función predic para hallar los valores ajustados y se calcula el MSE de este modelo.

```{r}
lasso.pred<-predict(lasso.mod, s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
```
Finalmente, con los valores predichos del modelo se eliminan todos aquellos iguales a cero para ver cuales son las covariables que podrían explicar mejor a $Y$.

```{r}
out<-glmnet (x,y,alpha=1)
lasso.coef<-predict(out,type="coefficients",s=bestlam)[1:1328,]
lasso.coef[lasso.coef!=0]
```
Se observa que de 1328 variables solo el intercepto, $X_6$, $X_{94}$, $X_{119}$, $X_{1000}$ y $X_{1263}$ son candidatas a entrar en el modelo para explicar de la mejor manera a la variable respuesta.

Por último, las variables que aparentemente muestran no ser relevantes para explicar la variable aleatoria $Y$ son las que no fueron incluidas en el conjunto anterior.

\textbf{Base 2 }

\textbf{Regresión ridge} 

```{r}
base2 <- read.table("../BASE_DATOS_2.txt", header = T)
```

```{r}
base2<-na.omit(base2) 
x<-model.matrix(Y~.,base2)[,-1] 
y<-base2$Y
#rejilla para los lambda.
gridz<-10^seq(-2,10, length=100) 
ridge.mod<-glmnet(x,y,alpha=0, lambda=gridz)
dim(coef(ridge.mod))
```

```{r}
plot(ridge.mod, xvar="lambda", label=TRUE)
```
El modelo se vuelve menos flexible de izquierda a derecha en aproximadamente los parámetros del modelo son casi cero a partir de log(6). 

```{r}
cedula<-1233
set.seed(cedula)
train<-sample(1: nrow(x), nrow(x)/2)
test<- -train
y.test<-y[test] 
cv.out<-cv.glmnet(x[train,],y[train],alpha=0)
```

```{r}
plot(cv.out)
```

```{r}
 bestlam<-cv.out$lambda.min ;bestlam
```

Del resultado anterior se puede concluir que lambda es aproxmadamente 0.9577

```{r}
ridge.pred<-predict(ridge.mod, s=bestlam,newx=x[test,] )
mean((ridge.pred-y.test)^2)
```

Se obtiene un MSE de prueba de aproximadamente 1.778. A continuación se muestran algunos de los coeficientes estimados con todos los datos. Sus variables corresponden a las más reelevantes para explicar a Y. Las demás, en este caso no se considerarán reelevantes.

```{r}
out<-glmnet (x,y,alpha=0) 
pre <- predict(out,type="coefficients",s=bestlam)[1:147,]
sort(abs(pre), decreasing = T)[1:21]
```



\textbf{Base 2 - regresión lasso} 
 
 
```{r}
x<-model.matrix(Y~.,base2)[,-1] 
y<-base2$Y
gridz<-10^seq(-2,10, length=100) 
lasso.mod<-glmnet(x,y,alpha=1, lambda=gridz)
dim(coef(lasso.mod))
```

En el siguiente grafico podemos observar cuales son la variables que más están siendo impactadas por la regularización.

```{r}
plot(lasso.mod, xvar="lambda", label=TRUE)
```

```{r}
cedula<-1233
set.seed(cedula)
train<-sample(1: nrow(x), nrow(x)/2)
test<- -train
y.test<-y[test] 
cv.out<-cv.glmnet(x[train,],y[train],alpha=1)
```

```{r}
plot(cv.out)
```
A continuación se obtiene el lambda con el menor error cuadrático medio MSE, es cual es 0.027

```{r}
 bestlam<-cv.out$lambda.min ;bestlam
```

Se obtiene el MSE con el conjunto de prueba:

```{r}
lasso.pred<-predict(lasso.mod, s=bestlam,newx=x[test,]) 
mean((lasso.pred-y.test)^2)
```

Se procede a ajustar con todos los datos y obtener algunos de los coeficientes.

```{r}
out<-glmnet (x,y,alpha=1) 
lasso.coef<-predict(out,type="coefficients",s=bestlam)[1:147,] 
lasso.coef[lasso.coef!=0]
```

Se concluye entonces que las variables más reelevantes para explicar a Y son x18          x43, x48, x55, x82, x101 y el intercepto. Las demás no se consideran reelevantes.




   